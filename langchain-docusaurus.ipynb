{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654c6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import DocusaurusLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4994327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixes a bug with asyncio and jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3d0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 205/205 [00:09<00:00, 21.29it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DocusaurusLoader(\n",
    "    \"https://crawlee.dev/python/\",\n",
    "    filter_urls=[\n",
    "        \"https://crawlee.dev/python/api\"\n",
    "    ],\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0fffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://crawlee.dev/python/api/class/BeautifulSoupCrawler', 'loc': 'https://crawlee.dev/python/api/class/BeautifulSoupCrawler', 'changefreq': 'weekly', 'priority': '0.5'}, page_content='BeautifulSoupCrawler | API | Crawlee for Python · Fast, reliable Python web crawlers.Skip to main contentCrawlee for PythonDocsExamplesAPIChangelogBlogNextNext2.21.3Search documentation...Get startedCrawlee for PythonOverviewClassesAdaptivePlaywrightCrawlerAutoscaledPoolBasicCrawlerBeautifulSoupCrawlerBrowserforgeFingerprintGeneratorBrowserPoolContextPipelineCurlImpersonateHttpClientDatasetDefaultRenderingTypePredictorEventManagerHeaderGeneratorHttpCrawlerHttpxHttpClientKeyValueStoreLocalEventManagerMemoryStorageClientParselCrawlerPlaywrightBrowserControllerPlaywrightBrowserPluginPlaywrightCrawlerProxyConfigurationRenderingTypePredictorRequestListRequestManagerTandemRequestQueueRouterServiceLocatorSessionPoolSnapshotterStatisticsSystemStatusAbstract classesData structuresErrorsEvent payloadsFunctionsClassesBeautifulSoupCrawlerOn this pageBeautifulSoupCrawler A web crawler for performing HTTP requests and parsing HTML/XML content.\\nThe BeautifulSoupCrawler builds on top of the AbstractHttpCrawler, which means it inherits all of its features.\\nIt specifies its own parser BeautifulSoupParser which is used to parse HttpResponse.\\nBeautifulSoupParser uses following library for parsing: https://pypi.org/project/beautifulsoup4/\\nThe HTTP client-based crawlers are ideal for websites that do not require JavaScript execution. However,\\nif you need to execute client-side JavaScript, consider using browser-based crawler like the PlaywrightCrawler.\\nUsagefrom crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContextcrawler = BeautifulSoupCrawler()# Define the default request handler, which will be called for every request.@crawler.router.default_handlerasync def request_handler(context: BeautifulSoupCrawlingContext) -> None:    context.log.info(f\\'Processing {context.request.url} ...\\')    # Extract data from the page.    data = {        \\'url\\': context.request.url,        \\'title\\': context.soup.title.string if context.soup.title else None,    }    # Push the extracted data to the default dataset.    await context.push_data(data)await crawler.run([\\'https://crawlee.dev/\\'])HierarchyAbstractHttpCrawlerBeautifulSoupCrawlerIndex Methods__init__add_requestscreate_parsed_http_crawler_classerror_handlerexport_dataexport_data_csvexport_data_jsonfailed_request_handlerget_dataget_datasetget_key_value_storeget_request_manageron_skipped_requestpre_navigation_hookrunstopPropertieslogrouterstatisticsMethods __init__ __init__(*, parser, request_handler, statistics, configuration, event_manager, storage_client, request_manager, session_pool, proxy_configuration, http_client, max_request_retries, max_requests_per_crawl, max_session_rotations, max_crawl_depth, use_session_pool, retry_on_blocked, concurrency_settings, request_handler_timeout, abort_on_error, configure_logging, statistics_log_format, keep_alive, additional_http_error_status_codes, ignore_http_error_status_codes, respect_robots_txt_file): NoneOverrides AbstractHttpCrawler.__init__Initialize a new instance.Parametersoptionalkeyword-onlyparser: BeautifulSoupParserType = \\'lxml\\'The type of parser that should be used by BeautifulSoup.keyword-onlyoptionalrequest_handler: Callable[[TCrawlingContext], Awaitable[None]]A callable responsible for handling requests.keyword-onlyoptionalstatistics: Statistics[TStatisticsState]A custom Statistics instance, allowing the use of non-default configuration.keyword-onlyoptionalconfiguration: ConfigurationThe Configuration instance. Some of its properties are used as defaults for the crawler.keyword-onlyoptionalevent_manager: EventManagerThe event manager for managing events for the crawler and all its components.keyword-onlyoptionalstorage_client: StorageClientThe storage client for managing storages for the crawler and all its components.keyword-onlyoptionalrequest_manager: RequestManagerManager of requests that should be processed by the crawler.keyword-onlyoptionalsession_pool: SessionPoolA custom SessionPool instance, allowing the use of non-default configuration.keyword-onlyoptionalproxy_configuration: ProxyConfigurationHTTP proxy configuration used when making requests.keyword-onlyoptionalhttp_client: HttpClientHTTP client used by BasicCrawlingContext.send_request method.keyword-onlyoptionalmax_request_retries: intMaximum number of attempts to process a single request.keyword-onlyoptionalmax_requests_per_crawl: int | NoneMaximum number of pages to open during a crawl. The crawl stops upon reaching this limit.\\nSetting this value can help avoid infinite loops in misconfigured crawlers. None means no limit.\\nDue to concurrency settings, the actual number of pages visited may slightly exceed this value.keyword-onlyoptionalmax_session_rotations: intMaximum number of session rotations per request. The crawler rotates the session if a proxy error occurs\\nor if the website blocks the request.keyword-onlyoptionalmax_crawl_depth: int | NoneSpecifies the maximum crawl depth. If set, the crawler will stop processing links beyond this depth.\\nThe crawl depth starts at 0 for initial requests and increases with each subsequent level of links.\\nRequests at the maximum depth will still be processed, but no new links will be enqueued from those requests.\\nIf not set, crawling continues without depth restrictions.keyword-onlyoptionaluse_session_pool: boolEnable the use of a session pool for managing sessions during crawling.keyword-onlyoptionalretry_on_blocked: boolIf True, the crawler attempts to bypass bot protections automatically.keyword-onlyoptionalconcurrency_settings: ConcurrencySettingsSettings to fine-tune concurrency levels.keyword-onlyoptionalrequest_handler_timeout: timedeltaMaximum duration allowed for a single request handler to run.keyword-onlyoptionalabort_on_error: boolIf True, the crawler stops immediately when any request handler error occurs.keyword-onlyoptionalconfigure_logging: boolIf True, the crawler will set up logging infrastructure automatically.keyword-onlyoptionalstatistics_log_format: Literal[table, inline]If \\'table\\', displays crawler statistics as formatted tables in logs. If \\'inline\\', outputs statistics as plain\\ntext log messages.keyword-onlyoptionalkeep_alive: boolFlag that can keep crawler running even when there are no requests in queue.keyword-onlyoptionaladditional_http_error_status_codes: Iterable[int]Additional HTTP status codes to treat as errors, triggering automatic retries when encountered.keyword-onlyoptionalignore_http_error_status_codes: Iterable[int]HTTP status codes that are typically considered errors but should be treated as successful responses.keyword-onlyoptionalrespect_robots_txt_file: boolIf set to True, the crawler will automatically try to fetch the robots.txt file for each domain,\\nand skip those that are not allowed. This also prevents disallowed URLs to be added via EnqueueLinksFunction.Returns Noneadd_requestsasync add_requests(requests, *, batch_size, wait_time_between_batches, wait_for_all_requests_to_be_added, wait_for_all_requests_to_be_added_timeout): NoneInherited from BasicCrawler.add_requestsAdd requests to the underlying request manager in batches.Parametersrequests: Sequence[str | Request]A list of requests to add to the queue.optionalkeyword-onlybatch_size: int = 1000The number of requests to add in one batch.optionalkeyword-onlywait_time_between_batches: timedelta = timedelta(0)Time to wait between adding batches.optionalkeyword-onlywait_for_all_requests_to_be_added: bool = FalseIf True, wait for all requests to be added before returning.optionalkeyword-onlywait_for_all_requests_to_be_added_timeout: timedelta | None = NoneTimeout for waiting for all requests to be added.Returns Nonecreate_parsed_http_crawler_class create_parsed_http_crawler_class(static_parser): type[AbstractHttpCrawler[ParsedHttpCrawlingContext[TParseResult], TParseResult, TSelectResult]]Inherited from AbstractHttpCrawler.create_parsed_http_crawler_classCreate a specific version of AbstractHttpCrawler class.\\nThis is a convenience factory method for creating a specific AbstractHttpCrawler subclass.\\nWhile AbstractHttpCrawler allows its two generic parameters to be independent,\\nthis method simplifies cases where TParseResult is used for both generic parameters.Parametersstatic_parser: AbstractHttpParser[TParseResult, TSelectResult]Returns type[AbstractHttpCrawler[ParsedHttpCrawlingContext[TParseResult], TParseResult, TSelectResult]]error_handler error_handler(handler): ErrorHandler[TCrawlingContext]Inherited from BasicCrawler.error_handlerRegister a function to handle errors occurring in request handlers.\\nThe error handler is invoked after a request handler error occurs and before a retry attempt.Parametershandler: ErrorHandler[TCrawlingContext | BasicCrawlingContext]Returns ErrorHandler[TCrawlingContext]export_dataasync export_data(path, dataset_id, dataset_name): NoneInherited from BasicCrawler.export_dataExport data from a Dataset.\\nThis helper method simplifies the process of exporting data from a Dataset. It opens the specified\\none and then exports the data based on the provided parameters. If you need to pass options\\nspecific to the output format, use the export_data_csv or export_data_json method instead.Parameterspath: str | PathThe destination path.optionaldataset_id: str | None = NoneThe ID of the Dataset.optionaldataset_name: str | None = NoneThe name of the Dataset.Returns Noneexport_data_csvasync export_data_csv(path, *, dataset_id, dataset_name, dialect, delimiter, doublequote, escapechar, lineterminator, quotechar, quoting, skipinitialspace, strict): NoneInherited from BasicCrawler.export_data_csvExport data from a Dataset to a CSV file.\\nThis helper method simplifies the process of exporting data from a Dataset in csv format. It opens\\nthe specified one and then exports the data based on the provided parameters.Parameterspath: str | PathThe destination path.optionalkeyword-onlydataset_id: str | None = NoneThe ID of the Dataset.optionalkeyword-onlydataset_name: str | None = NoneThe name of the Dataset.keyword-onlyoptionaldialect: strSpecifies a dialect to be used in CSV parsing and writing.keyword-onlyoptionaldelimiter: strA one-character string used to separate fields. Defaults to \\',\\'.keyword-onlyoptionaldoublequote: boolControls how instances of quotechar inside a field should be quoted. When True, the character is doubled;\\nwhen False, the escapechar is used as a prefix. Defaults to True.keyword-onlyoptionalescapechar: strA one-character string used to escape the delimiter if quoting is set to QUOTE_NONE and the quotechar\\nif doublequote is False. Defaults to None, disabling escaping.keyword-onlyoptionallineterminator: strThe string used to terminate lines produced by the writer. Defaults to \\'\\\\r\\\\n\\'.keyword-onlyoptionalquotechar: strA one-character string used to quote fields containing special characters, like the delimiter or quotechar,\\nor fields containing new-line characters. Defaults to \\'\"\\'.keyword-onlyoptionalquoting: intControls when quotes should be generated by the writer and recognized by the reader. Can take any of\\nthe QUOTE_* constants, with a default of QUOTE_MINIMAL.keyword-onlyoptionalskipinitialspace: boolWhen True, spaces immediately following the delimiter are ignored. Defaults to False.keyword-onlyoptionalstrict: boolWhen True, raises an exception on bad CSV input. Defaults to False.Returns Noneexport_data_jsonasync export_data_json(path, *, dataset_id, dataset_name, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys): NoneInherited from BasicCrawler.export_data_jsonExport data from a Dataset to a JSON file.\\nThis helper method simplifies the process of exporting data from a Dataset in json format. It opens the\\nspecified one and then exports the data based on the provided parameters.Parameterspath: str | PathThe destination pathoptionalkeyword-onlydataset_id: str | None = NoneThe ID of the Dataset.optionalkeyword-onlydataset_name: str | None = NoneThe name of the Dataset.keyword-onlyoptionalskipkeys: boolIf True (default: False), dict keys that are not of a basic type (str, int, float, bool, None) will be skipped\\ninstead of raising a TypeError.keyword-onlyoptionalensure_ascii: boolDetermines if non-ASCII characters should be escaped in the output JSON string.keyword-onlyoptionalcheck_circular: boolIf False (default: True), skips the circular reference check for container types. A circular reference will\\nresult in a RecursionError or worse if unchecked.keyword-onlyoptionalallow_nan: boolIf False (default: True), raises a ValueError for out-of-range float values (nan, inf, -inf) to strictly comply\\nwith the JSON specification. If True, uses their JavaScript equivalents (NaN, Infinity, -Infinity).keyword-onlyoptionalcls: type[json.JSONEncoder]Allows specifying a custom JSON encoder.keyword-onlyoptionalindent: intSpecifies the number of spaces for indentation in the pretty-printed JSON output.keyword-onlyoptionalseparators: tuple[str, str]A tuple of (item_separator, key_separator). The default is (\\', \\', \\': \\') if indent is None and (\\',\\', \\': \\')\\notherwise.keyword-onlyoptionaldefault: CallableA function called for objects that can\\'t be serialized otherwise. It should return a JSON-encodable version\\nof the object or raise a TypeError.keyword-onlyoptionalsort_keys: boolSpecifies whether the output JSON object should have keys sorted alphabetically.Returns Nonefailed_request_handler failed_request_handler(handler): FailedRequestHandler[TCrawlingContext]Inherited from BasicCrawler.failed_request_handlerRegister a function to handle requests that exceed the maximum retry limit.\\nThe failed request handler is invoked when a request has failed all retry attempts.Parametershandler: FailedRequestHandler[TCrawlingContext | BasicCrawlingContext]Returns FailedRequestHandler[TCrawlingContext]get_dataasync get_data(dataset_id, dataset_name, *, offset, limit, clean, desc, fields, omit, unwind, skip_empty, skip_hidden, flatten, view): DatasetItemsListPageInherited from BasicCrawler.get_dataRetrieve data from a Dataset.\\nThis helper method simplifies the process of retrieving data from a Dataset. It opens the specified\\none and then retrieves the data based on the provided parameters.Parametersoptionaldataset_id: str | None = NoneThe ID of the Dataset.optionaldataset_name: str | None = NoneThe name of the Dataset.keyword-onlyoptionaloffset: intSkip the specified number of items at the start.keyword-onlyoptionallimit: intThe maximum number of items to retrieve. Unlimited if None.keyword-onlyoptionalclean: boolReturn only non-empty items and excludes hidden fields. Shortcut for skip_hidden and skip_empty.keyword-onlyoptionaldesc: boolSet to True to sort results in descending order.keyword-onlyoptionalfields: list[str]Fields to include in each item. Sorts fields as specified if provided.keyword-onlyoptionalomit: list[str]Fields to exclude from each item.keyword-onlyoptionalunwind: strUnwind items by a specified array field, turning each element into a separate item.keyword-onlyoptionalskip_empty: boolExclude empty items from the results if True.keyword-onlyoptionalskip_hidden: boolExclude fields starting with \\'#\\' if True.keyword-onlyoptionalflatten: list[str]Field to be flattened in returned items.keyword-onlyoptionalview: strSpecify the dataset view to be used.Returns DatasetItemsListPageget_datasetasync get_dataset(*, id, name): DatasetInherited from BasicCrawler.get_datasetReturn the Dataset with the given ID or name. If none is provided, return the default one.Parametersoptionalkeyword-onlyid: str | None = Noneoptionalkeyword-onlyname: str | None = NoneReturns Datasetget_key_value_storeasync get_key_value_store(*, id, name): KeyValueStoreInherited from BasicCrawler.get_key_value_storeReturn the KeyValueStore with the given ID or name. If none is provided, return the default KVS.Parametersoptionalkeyword-onlyid: str | None = Noneoptionalkeyword-onlyname: str | None = NoneReturns KeyValueStoreget_request_managerasync get_request_manager(): RequestManagerInherited from BasicCrawler.get_request_managerReturn the configured request manager. If none is configured, open and return the default request queue.Returns RequestManageron_skipped_request on_skipped_request(callback): SkippedRequestCallbackInherited from BasicCrawler.on_skipped_requestRegister a function to handle skipped requests.\\nThe skipped request handler is invoked when a request is skipped due to a collision or other reasons.Parameterscallback: SkippedRequestCallbackReturns SkippedRequestCallbackpre_navigation_hook pre_navigation_hook(hook): NoneInherited from AbstractHttpCrawler.pre_navigation_hookRegister a hook to be called before each navigation.Parametershook: Callable[[BasicCrawlingContext], Awaitable[None]]A coroutine function to be called before each navigation.Returns Nonerunasync run(requests, *, purge_request_queue): FinalStatisticsInherited from BasicCrawler.runRun the crawler until all requests are processed.Parametersoptionalrequests: Sequence[str | Request] | None = NoneThe requests to be enqueued before the crawler starts.optionalkeyword-onlypurge_request_queue: bool = TrueIf this is True and the crawler is not being run for the first time, the default\\nrequest queue will be purged.Returns FinalStatisticsstop stop(reason): NoneInherited from BasicCrawler.stopSet flag to stop crawler.\\nThis stops current crawler run regardless of whether all requests were finished.Parametersoptionalreason: str = \\'Stop was called externally.\\'Reason for stopping that will be used in logs.Returns NoneProperties loglog:  logging.LoggerInherited from BasicCrawler.logThe logger used by the crawler.routerrouter:  Router[TCrawlingContext]Inherited from BasicCrawler.routerOverrides BasicCrawler.routerThe Router used to handle each individual crawling request.statisticsstatistics:  Statistics[TStatisticsState]Inherited from BasicCrawler.statisticsStatistics about the current (or last) crawler run.PreviousBasicCrawlerNextBrowserforgeFingerprintGeneratorPage OptionsHide Inherited __init__ add_requests create_parsed_http_crawler_class error_handler export_data export_data_csv export_data_json failed_request_handler get_data get_dataset get_key_value_store get_request_manager on_skipped_request pre_navigation_hook run stop log router statisticsDocsGuidesExamplesAPI referenceChangelogProductDiscordStack OverflowTwitterYouTubeMoreApify platformDocusaurusGitHubCrawlee is forever free and open source© 2025 Apify\\n\\n\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
